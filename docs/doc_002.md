### 刊首语

这里记录ML自学者群体，每周分享优秀的学习心得与资料。由于微信不允许外部链接，需要点击文末的「阅读原文」，才能访问文中的链接。

---

### 本期内容

**目录**

**一、学习心得分享**
- 图像超分辨和图像高分辨的区别
- 挖掘模板，辅助对话生成
- 学习梯度优化算法

**二、机器学习解答**
- 为什么SVM不会过拟合
- GBDT和XGBOOST的区别有哪些
- 如何入门NLP，比较茫然

**三、资料整理**
- 论文类
- 工具类

---

### 学习心得分享

#### 图像超分辨和图像高分辨的区别

图像超分辨和图像高分辨之间的区别是：**图像超分辨描述**是图像由小到大的一个变化过程，**图像高分辨**是图像本身较大的一个状态。对于图图像的超分辨任务SR通常使用比较深的卷积神经网络编码成较高分辨率的图像过程，该方向比较难的一个点是SISR单张图像超分辨在医学、天文、安防方面有广泛使用，通常都是插值重构的方案，但是这种方法有一定局限性。

代码地址：https://github.com/lightningsoon/Residual-Dense-Net-for-Super-Resolution

论文地址：https://arxiv.org/abs/1802.08797

在上面的论文和代码中是以三个模块实现超分辨的单张功能，模块一是密集型残差网络RDB、模块二是局部特征混合LFF、模块三是LRL局部特征学习。因为卷积神经网络和图像中距离像素较远的关联关系较弱所以局部特征更能表达深度特征能力。整个RDN主要包括4个部分：隐藏特征提取网络（SFENet），残差密集模块（RDBs），密集特征融合（DFF），和最后的上采样网络（UPNet）。

#### 挖掘模板，辅助对话生成

EMNLP2019的一篇文章，对话领域中，训练语料（通常指的是post-response的语句对形式）较难采集。而无序的语句较容易获得。利用这些大量无监督语料，来提升机器对话回复的质量，是本文提出新的探索方向。作者将大量无序语句和少量对话语句对相结合，从无序语句中学习对话模板（template），模板涵盖了语义和语法的信息，作为先验知识，从而辅助对话response的生成。数据集采用了微博和知乎的语料，模型结构如下所示。

论文名称：Low-Resource Response Generation with Template Prior

论文地址：https://arxiv.org/abs/1909.11968


#### 学习梯度优化算法

这篇博客是梯度优化算法综述，附上一些读后感。
1. SGD : 每次朝着梯度的反方向进行更新。
2. Momentum：动能积累，每次更新时候积累 SGD 的更新方向，也就是每次参数更新时，不单单考虑当前的梯度方向，还考虑之前的梯度方向，当然之前的梯度会有一个衰减因子。
3. Adagrad：自动调节学习率大小，对于频繁更新的参数，学习率小；对于很少更新的参数，学习率大。怎么判断参数频繁更新呢？可以为更新公式设置一个分母：梯度平方累加和的平方根。这样当分母越大，说明之前该参数越频繁更新，反之则很少更新。存在问题：当迭代次数多了之后，由于是累加操作，分母越来越大，学习率会变得非常小，模型更新会很慢。
4. Adadelta：作为 Adagrad 的拓展，是解决学习率消失的问题。在这里不会直接叠加之前所有的梯度平方，而是引入了一个梯度衰退的因子，使得时间久远的梯度对此刻参数更新的影响会消失。RMSProp 的思想和其类似。
5. Adam：结合 Momentum 和 Adadelta 两种优化算法的优点。对梯度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second Moment Estimation，即梯度的未中心化的方差）进行综合考虑，计算出更新步长。

文章地址：http://ruder.io/optimizing-gradient-descent/

---

### 机器学习问答

#### [为什么svm不会过拟合](https://www.zhihu.com/question/20178589/answer/29950675)

SVM当然会过拟合，而且过度拟合的能力还非常强。首先我想说说什么叫过度拟合？就是模型学岔路了，把数据中的噪音当做有效的分类标准。

通常越是描述能力强大的模型越容易过度拟合。描述能力强的模型就像聪明人，描述能力弱的如：”一次线性模型“像傻子，如果聪明人要骗人肯定比傻子更能自圆其说对不对？而SVM的其中一个优化目标：最小化||W||，就是抑制它的描述能力，聪明人是吧，只允许你用100个字，能把事情说清楚不？

这就是为什么regularization能够对抗过度拟合，同时它也在弱化模型的描述能力。但只要能说话就能说谎，就能歪曲事实对不对？别把SVM想得太复杂，你就可以把它当做一个线性分类器，只不过它优化了分类平面与分类数据之间距离。

#### [GBDT和XGBOOST的区别有哪些](https://zhuanlan.zhihu.com/p/30316845)

1. 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
2. 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。
3. Xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
4. Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
5. 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。

#### 如何入门NLP，比较茫然

先了解什么是词向量 onehot和 word2vec，然后了解最基础的文本分类任务，跑下经典的机器学习模型和现在的cnn、rnn分类模型。这些代码都不多的，用pytorch或者tf实现下挺快的，可以照着代码敲一遍，有感觉了就离入门近一些了。再然后，看看nlp常见的任务，ner，mrc，nli，等等。对了，推荐直接跟着斯坦福的nlp公开课走一遍吧，很快的。

课程名：CS224d - Deep Learning for Natural Language Processing

课程地址：http://cs224d.stanford.edu

---

### 资料整理

#### 论文类
- [APDrawingGAN：人脸秒变艺术肖像画](https://mp.weixin.qq.com/s/Ok9ediwb35LzQiT9YVKfWA)
- [NeurIPS 2019 | 用于弱监督图像语义分割的新型损失函数](https://mp.weixin.qq.com/s/CbORYhJQn27J0G4G6XpODw)
- [Doc2EDAG：一种针对中文金融事件抽取的端到端文档级框架](https://mp.weixin.qq.com/s/irYEpq9pkeZYoSRcp4auew)

#### 工具类
- [三行代码提取PDF表格数据](https://mp.weixin.qq.com/s/VOU9bZTYENI0wZnP9SmAtQ)
- [PyTorch 1.3发布：能在移动端部署](https://mp.weixin.qq.com/s/NNTA7B_ZZNruh01Nax26Mg)
- [谷歌工程师：Tensorflow2.0 简单粗暴教程中文版](https://mp.weixin.qq.com/s/sG2Xp0vLzlW5zE1k7myB4w)

---

### 加入我们

公众号内回复「自学」，即可加入ML自学者俱乐部社群。可以投稿每周学习心得或者看到的优质学习资料，助力团体共同学习进步。

---

### 参考来源

- [ML自学者俱乐部投稿](https://github.com/Dikea/ML-SelfStudy-Weekly)
- [黄博的机器学习圈子](https://t.zsxq.com/eaeYv7a)
- [知乎机器学习话题](https://zhihu.com)

---

[点击阅读上一期内容](https://mp.weixin.qq.com/s/aqn1jN1_ZqC_KAsSLG9SYg)

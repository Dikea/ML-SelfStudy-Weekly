> 这里记录自学者的学习内容，欢迎留言投稿你的自学内容。

### 刊首语

创刊第 1 期，会一直坚持下去，希望能够做到 100 期。一直以来，同学们都在坚持分享。好的自学内容，还是放出来让大家多看看、多交流为好。欢迎投稿，每周一起学习进步！

#### 学习ALBERT

ALBERT A LITE BERT：是一个轻量级的 BERT 模型，和BERT比有三个变化点：
- 嵌入向量参数化的因式分解不再将 one-hot 向量直接映射到大小为 H 的隐藏空间，而是先将它们映射到一个低维词嵌入空间 E，然后再映射到隐藏空间。通过这种分解，研究者可以将词嵌入参数从 O(V × H) 降低到 O(V × E + E × H)，这在 H 远远大于 E 的时候，参数量减少得非常明显，减少计算量，加快计算时间。
- 跨层参数共享：所有层权重共享
- 句间连贯性损失：句间建模使用基于语言连贯性的损失函数。对于 ALBERT，研究者使用了一个句子顺序预测（SOP）损失函数，它会避免预测主题，而只关注建模句子之间的连贯性。

#### 目标跟踪 PRCF

本文研究了池操作对视觉跟踪的影响，提出了一种新的ROI池相关滤波算法。虽然基于roi的池算法在许多基于深度学习的应用中得到了成功的应用，但是在视觉跟踪领域，尤其是在基于相关滤波的方法中，却很少考虑到它。由于相关滤波公式并不能真正提取出正样本和负样本，所以快速R-CNN等基于roi的池是不可行的。通过数学推导，给出了实现基于roi的池的另一种解决方案。提出了一种具有等式约束的相关滤波算法，通过该算法可以等价地实现基于roi的池。提出了一种求解优化问题的交替方向乘法器(ADMM)算法，并在傅里叶域中给出了一种有效的求解方法。
论文阅读笔记：http://haha-strong.com/2019/09/23/20190923-RoiCF/

#### 大数据系统工程架构
![](https://mmbiz.qpic.cn/mmbiz_jpg/ZVbHwHpkBy8icS8NicN5P2yTjsRicVrC640tL9HicobPZ1NPaw8AcWEk5o9zD8ch78nIjeiclwJQachV5CMTvlmaMlA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### 温习XGBoost

最近温习了 XGBoost，通过重读论文，阅读一些公众号的推送和博客，重新把公式推导了一遍，详细了解损失函数，泰勒展开，节点分裂，如果选择最优划分等具体过程，此外，了解了一些并行化的处理方式。 
- [公众号文章：XGBoost超详细推导，终于有人讲明白了！](https://mp.weixin.qq.com/s/7n1nzGL7r789P9sv0GEkDA)
- 论文：https://arxiv.org/pdf/1603.02754.pdf
- PPT：https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf
- 并行处理：[Parallel Gradient Boosting Decision Trees](http://zhanpengfang.github.io/418home.html)

#### 小样本学习

最近老师给的任务涉及到小样本学习问题，读了以下针对小样本学习的MTL算法，交流一下我学到的东西，理解不准确的地方还望指正。

Meta-Transfer Learning for Few-Shot Learning是CVPR 2019接收的论文，第一作者是新加坡国立大学的Qianru Sun，根据Papers With Code这个网站的评估，该算法截止到今天是SOTA for Few-Shot Image Classification on Fewshot-CIFAR100 - 10-Shot Learning。

元学习的架构已经被提出，并广泛应用到小样本检测问题上，元学习的核心是利用大量相似的小样本检测任务，以学习如何去适应一个新的小样本学习任务。传统的DNN网络在处理小样本学习问题时会有过拟合的问题，因此元学习通常使用浅层神经网络，但是这也限制了网络的性能。针对上述问题，这篇文章提出了一种新型的小样本检测算法，叫做MTL，它采用了一种深层神经网络用于小样本检测问题。M是指meta，代表着多种多样的任务，T是指transfer，通过学习每个任务的DNN权重的缩放和移位功能，可以实现权重的传递。

除此之外，这篇文章介绍了一种方法，该方法对于提升算法性能非常有帮助。传统的元学习方法受到两方面的限制：
- 这些方法需要大量类似的任务来进行元训练，而找到大量相似任务是非常困难的；
- 每个任务通常由低复杂度的浅层神经网络构成，以避免模型出现过拟合，因此无法使用更深更强大的体系结构。

1. 第一步：训练DNN网络在大尺度数据集上，并且将低层固定为特征提取器，需要注意的是，迁移给小样本学习任务的特征提取器的相关权重而不是DNN的最后一层权重。
2. 第二步：元迁移学习阶段，MTL学习特征提取神经元的缩放和移位参数，从而能够快速适应Few-shot Learning任务。具体实现过程在论文的4.2节，讲道理我只是看懂了一部分，相比于传统的方法，这篇论文在传递到小样本学习任务时，冻结迁移过来的权重，不进行更新，而其他的相关权重正常进行更新，感觉这篇文章的精髓在于这一部分的冻结操作和迁移过程的精妙操作。具体怎么迁移的，还需在继续学习下。
3. 第三步：为了提升整体学习水平，使用HT元批量学习策略。HT元批量学习策略是指挑选检测失败的案例进行附加训练，重点强调识别错误的例子，“在失败中成长”…，根据本文的试验，效果还不错。

- 文章下载地址：https://arxiv.org/pdf/1812.02391v3.pdf
- 文章源代码：https://github.com/y2l/meta-transfer-learning-tensorflow

#### 推公式

路漫漫其修远兮，吾将上下而求索。手推牛顿法，混合高斯模型，SVM，核方法，EM，CRF，MCMC等等。这里极度推荐b站shuhuai的视频，里面的公式解析极其细致！
![](https://mmbiz.qpic.cn/mmbiz_png/ZVbHwHpkBy8icS8NicN5P2yTjsRicVrC6403nbdvFHsTjgwXMX1SkV26G3CwyicSYiaZNwAguORO5OtqVZ2s2SrjZPQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
